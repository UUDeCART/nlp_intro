{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Agreement for Brat annotations\n",
    "\n",
    "Now we have your annotations ready and have learned the agreement formulas, let's try some exercises to calculate the agreement betwee each other.\n",
    "\n",
    "Although the formulas are simple, efficiently getting the numbers in the contingency table is not trivial. We have provided an optimized function for you here (If you are interested how we implemented it, check [here](./compare_utils.py). ). Let's try it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "from compare_utils import compare_projects,show_annotations\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initiate the directories and read the annotations\n",
    "\n",
    "First, we need to tell compare who against who. In Brat, annotations are saved in directories, thus the question is equivalent to compare which directory against which.\n",
    "\n",
    "If you are not sure what directories you should look for, check the list here:\n",
    "https://brat.jupyter.med.utah.edu/#/student_folders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell where is the projects located, you need to replace them with your project name and reference project name\n",
    "annotator_a='JIANLINS'\n",
    "annotator_b='goldstandard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the project name to real directory path\n",
    "\n",
    "brat_projects_loc=os.path.abspath('../../BRAT')\n",
    "annotator_a_dir=os.path.join(brat_projects_loc, annotator_a)\n",
    "annotator_b_dir=os.path.join(brat_projects_loc, annotator_b)\n",
    "\n",
    "# you could try to print annotator_a and annotator_b out to see where they are\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Strict comparison\n",
    "\n",
    "**compare_projects** is the function that we wrapped up the meat in. It takes in 2~3 paramters:\n",
    "1. Your directory \n",
    "2. The directory that you want to compare against\n",
    "3. compare method ('strict' or 'relax')\n",
    "\n",
    "It turns a dictionary of evaluators with annotation types as the key, an Evaluator as the value. The Evaluator class will contain all the numbers in the contingency table we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map, evaluators = compare_projects(annotator_a_dir, annotator_b_dir, 'strict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compare_projects** returns two values:\n",
    "1. *doc_map* contains a dictionary that maps a document name to its content text\n",
    "2. *evaluators* contains a dictionary that maps an annotation type to the corresponding compared results--an object of [Evaluator](./compare_utils.py)\n",
    "\n",
    "Next, let's take a look at what's inside evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for type_name, evaluator in evaluators.items():\n",
    "    print(type_name)\n",
    "    a,b,c,d=evaluator.get_values()\n",
    "#   now you can print these numbers\n",
    "    print(a,b,c,d)\n",
    "#   or display in a contingency table\n",
    "    display(evaluator.display_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can caculate your IAA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Relaxed comparsion\n",
    "When comparin mention level annotations, it is more useful to use relaxed comparision -- consider a match if an annotation of annotator A overlaps with the annotator B's. For instance, \"Left lower lobe pneumonia\" vs \"pneumonia\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code is very similar to the above\n",
    "doc_map,evaluators = compare_projects(annotator_a_dir, annotator_b_dir, 'relax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_name, evaluator in evaluators.items():\n",
    "    print(type_name)\n",
    "    a,b,c,d=evaluator.get_values()\n",
    "#   now you can print these numbers\n",
    "    print(a,b,c,d)\n",
    "#   or display in a contingency table\n",
    "    display(evaluator.display_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can try to calculate your IAA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to compare some types, here is the code you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map,evaluators = compare_projects(annotator_a_dir, annotator_b_dir, 'relax',['PNEUMONIA_DOC_NO','PNEUMONIA_DOC_YES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show the disagreement\n",
    "\n",
    "Now we are wondering where are the disagreement annotations. Evaluator saved that information as well. Let's try to display them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Show the annotations in annotator_a, but not annotator_b (false positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_name, evaluator in evaluators.items():\n",
    "    print(type_name)\n",
    "    print(evaluator.get_values())\n",
    "    fps=evaluator.get_fps()\n",
    "    show_annotations(fps, doc_map,annotator_a,annotator_b,900,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Show the annotations in annotator_b, but not annotator_a (false negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_name, evaluator in evaluators.items():\n",
    "    print(type_name)\n",
    "    fns=evaluator.get_fns()\n",
    "    print(evaluator.get_values())\n",
    "    show_annotations(fns, doc_map,annotator_a,annotator_b,900,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>This material presented as part of the DeCART Data Science for the Health Science Summer Program at the University of Utah in 2018.<br/>\n",
    "Presenters : Dr. Wendy Chapman, Jianlin Shi <br> Acknowledgement: Many thanks to Kelly Peterson and Olga Patterson because part of the materials are adopted from his previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
