{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Mention-Level Annotations to Document Classification\n",
    "\n",
    "## 1. Why do we need document classification?\n",
    "\n",
    "Think about a case with multiple mentions in one document. How do we decide the document level conclusion when these mentions have \"conflicted\" information? For example, \n",
    "\n",
    ">Small **left pleural effusion**. **Right pleural effusion can be excluded**.\n",
    "\n",
    "In this example, should we conclude that the report indicates pneumonia or does not indicate pneumonia?\n",
    "\n",
    "There are many other situations that we need to draw a document level conclusion based on multiple mention level annotations. Certainly, we can train a machine learning classifier to accomplish this task, which you will learn in another class. But here we are going to learn how to do it in rule-based way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Restore from where we are using pyConText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import everything that we will need\n",
    "import pyConTextNLP\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from pyConTextNLP.itemData import itemData\n",
    "from pyConTextNLP.utils import get_document_markups\n",
    "from pyConTextNLP.display.html import mark_document_with_html\n",
    "import os\n",
    "import os.path\n",
    "from nlp_pneumonia_utils import Annotation\n",
    "from nlp_pneumonia_utils import AnnotatedDocument\n",
    "from nlp_pneumonia_utils import read_brat_annotations\n",
    "from nlp_pneumonia_utils import read_annotations\n",
    "from nlp_pneumonia_utils import calculate_prediction_metrics\n",
    "from nlp_pneumonia_utils import markup_context_document\n",
    "from DocumentClassifier import DocumentClassifier\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "from visual import Vis\n",
    "from visual import snippets_markup\n",
    "from visual import view_pycontext_output\n",
    "from visual import convertMarkups2DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just consider the example at the beginning as a document,\n",
    "# and run pyConText to get markups\n",
    "\n",
    "report = \"Right pleural effusion can be excluded. Likely small left pleural effusion. \"\n",
    "\n",
    "targets = itemData([\"effusion\", \"EVIDENCE_OF_PNEUMONIA\", r\"effusion[s]?\", \"\"])\n",
    "\n",
    "modifiers = pyConTextNLP.itemData.instantiateFromCSVtoitemData(os.path.join(os.getcwd(),'KB/pneumonia_modifiers.tsv'))\n",
    "\n",
    "markups=markup_context_document(report,modifiers,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm what we get from pyConText\n",
    "view_pycontext_output(markups)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use DocumentClassifier to define the rules for document classification\n",
    "\n",
    "After we processed a document, we will get a list of pyConText output. But this is not the end. We want to conclude whether this document is pneumonia positive or not. That's where the DocumentClassifier comes in.\n",
    "\n",
    "### 3.1 Simple document classification rules\n",
    "First, let's start from two simple cases:\n",
    "1. If there is a true mention of pneumonia evidence, we should conclude \"PNEUMONIA_DOC_YES\"\n",
    "2. Otherwise, we should conclude \"PNEUMONIA_DOC_NO\"\n",
    "\n",
    "\n",
    "We can easily write some python code for that. Here we do something more----externalize the rule definitions, so that we can directly reuse our code for other projects. Here is an example rule file [doc_inferences.csv](../../../edit/work/decart_rule_based_nlp/KB/doc_inferences.csv):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "DocConclusion,EvidenceTypes\n",
    "# The rule in document inferences are processed from top to bottom.\n",
    "# If any one of the rules is matched, the rest rules below it will be skipped.\n",
    "# if the document has a EVIDENCE_OF_PNEUMONIA annotation, conclude PNEUMONIA_DOC_YES.\n",
    "PNEUMONIA_DOC_YES,EVIDENCE_OF_PNEUMONIA\n",
    "# if no above rule matched, conclude NEG_COLON_CA_DOC (default conclusion)\n",
    "PNEUMONIA_DOC_NO\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 How to integrate pyConText processed results\n",
    "However, the rules above haven't make any use of pyConText outputs. We definitely want to exclude the evidences that are negated. A simple idea to fix it is to go over all the annotations, change the negated \"EVIDENCE_OF_PNEUMONIA\" annotations to another annotation type, and then our DocumentClassifier can use the rules above to draw the conclusions correctly.\n",
    "\n",
    "Thus, we need another inference component (feature inferencer) to make this change. Its rule ([featurer_inferences.csv](../../../edit/work/decart_rule_based_nlp/KB/featurer_inferences.csv)) format is like following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "ConclusionType,SourceType,ModifierValues\n",
    "#if an annotation is 'EVIDENCE_OF_PNEUMONIA', only has a modifier \"DEFINITE_NEGATED_EXISTENCE', change this annotation type to NEG_EVIDENCE\n",
    "NEG_EVIDENCE,EVIDENCE_OF_PNEUMONIA,DEFINITE_NEGATED_EXISTENCE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dissect DocumentClassifier step by step\n",
    "To use these two sets of rules, we need to initiate our DocumentClassifier. This DocumentClassifier also wrapped pyConText inside to make it easier to use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_doc_type='PNEUMONIA_DOC_YES'\n",
    "TARGETS_FILE_PATH = 'KB/pneumonia_targets.tsv'\n",
    "MODIFIERS_FILE_PATH = 'KB/pneumonia_modifiers.tsv'\n",
    "FEATURE_INFERENCER_FILE_PATH = 'KB/featurer_inferences.csv'\n",
    "DOC_INFERENCER_FILE_PATH = 'KB/doc_inferences.csv'\n",
    "# clear just in case files/regular expressions have been updated\n",
    "classifier = DocumentClassifier(TARGETS_FILE_PATH, MODIFIERS_FILE_PATH,\n",
    "                               FEATURE_INFERENCER_FILE_PATH, DOC_INFERENCER_FILE_PATH,\n",
    "                               {pos_doc_type})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how this \"classifier\" gets to the conclusion step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st use pyConText to process the input text, \n",
    "context_doc = markup_context_document(report, classifier.modifiers, classifier.targets)\n",
    "view_pycontext_output(context_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read out the pyConText output into dataframe format\n",
    "annotations, relations, doc_txt = convertMarkups2DF(get_document_markups(context_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what is in annotations:\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And what is in \"relations\":\n",
    "relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use feature inference rules to change our negated annotations' type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_conclusion_types = classifier.feature_inferencer.process(annotations, relations)\n",
    "matched_conclusion_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the 1st annotation type has been changed to 'neg_evidence'. Next, we can draw document level conclusion by simply check if there still is any annotation of 'EVIDENCE_OF_PNEUMONIA':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_conclusion = classifier.document_inferencer.process(matched_conclusion_types)\n",
    "doc_conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Wrap up\n",
    "To make the call even simpler, DocumentClassifier has already wrapped up all the codes above into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_conclusion = classifier.classify_doc(report)\n",
    "doc_conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the last document that has been processed by it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_pycontext_output(classifier.get_last_context_doc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Excercise\n",
    "Let's try to switch the sentences in the example\n",
    "See what happens. Does the order of mention-level annotation affects final conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quiz\n",
    "Let's try a few questions, see if you've understood the content of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiz_utils import doc_classify_1\n",
    "doc_classify_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiz_utils import doc_classify_2\n",
    "doc_classify_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiz_utils import doc_classify_3\n",
    "doc_classify_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br/><br/>This material presented as part of the DeCART Data Science for the Health Science Summer Program at the University of Utah in 2018.<br/>\n",
    "Presenters : Dr.Wendy Chapman, Jianlin Shi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
